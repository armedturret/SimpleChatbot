{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitprogramdatavirtualenve74f98f206a144e9930fc7ad4501d43a",
   "display_name": "Python 3.7.4 64-bit ('ProgramData': virtualenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contains code under the Apache License, Version 2.0 copyright 2019 the TensorFlow Authors\n",
    "#sections of code have been modified for other usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#syntax operations on a given sentence to format it\n",
    "def preprocess(sentence):\n",
    "    return re.sub(r\"[^a-zA-Z?.!,\\']+\", \" \", re.sub(r'[\" \"]+', \" \", re.sub(r\"([?.!,])\", r\" \\1 \", sentence))).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#uses movie conversations and movie lines to create a (prompts, responses) tuple\n",
    "def create_data():\n",
    "    dialouge = open('./movie_lines.txt').read()\n",
    "    conversations = open('./movie_conversations.txt').read()\n",
    "    dialogue_data = {} #key val map with move linenumas the key and dialouge as the val\n",
    "\n",
    "    prompts, responses = [], []\n",
    "\n",
    "    #process the dialouge first\n",
    "    for l in dialouge.split('\\n'):\n",
    "        line = l.split(' +++$+++ ')\n",
    "        if not len(line) == 5: #skip if too short\n",
    "            continue\n",
    "        #treate puncation as sepereate by spacing them out and then removing all but the recognized characters\n",
    "        dialogue_data[(line[2] + line[0])] = preprocess(line[4])\n",
    "\n",
    "    #use the conversations file to assemble conversations\n",
    "    for l in conversations.split('\\n'):\n",
    "        line = l.split(' +++$+++ ')\n",
    "\n",
    "        if not len(line) == 4: #skip if too short\n",
    "            continue\n",
    "        #parse the last element as a list\n",
    "        lines = ast.literal_eval(line[3])\n",
    "        for i in range(1, len(lines)):\n",
    "            #assemble the lists\n",
    "            prompts.append(dialogue_data[line[2] + lines[i-1]])\n",
    "            responses.append(dialogue_data[line[2] + lines[i]])\n",
    "\n",
    "    return prompts, responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the data set creation function\n",
    "original_prompts, original_responses = create_data()\n",
    "\n",
    "#cut down the responses\n",
    "MAX_PAIRS = 1000\n",
    "original_prompts = original_prompts[:MAX_PAIRS]\n",
    "original_responses = original_responses[:MAX_PAIRS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Can we make this quick ? Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break up on the quad . Again . Well , I thought we'd start with pronunciation , if that's okay with you .\n"
     ]
    }
   ],
   "source": [
    "#print an example prompt/response\n",
    "print(original_prompts[0], original_responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create the tokenizer for the sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# handle everything including punctuation\n",
    "c_tokenizer = Tokenizer(filters='', oov_token='<OOV>', num_words=2**13)\n",
    "# fit on all data\n",
    "c_tokenizer.fit_on_texts(original_prompts + original_responses)\n",
    "\n",
    "START_TOKEN, END_TOKEN = [len(c_tokenizer.word_index)], [len(c_tokenizer.word_index) + 1]\n",
    "\n",
    "VOCAB_SIZE = len(c_tokenizer.word_index) + 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "st'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n<class 'list'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#create and pad the sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "tokenized_x = c_tokenizer.texts_to_sequences(original_prompts)\n",
    "tokenized_y = c_tokenizer.texts_to_sequences(original_responses)\n",
    "\n",
    "data_x, data_y = [], []\n",
    "\n",
    "#iterate through sequences making sure to only use sentences under max length\n",
    "for (question, answer) in zip(tokenized_x, tokenized_y):\n",
    "    if len(question) <= MAX_LENGTH and len(answer) <= MAX_LENGTH:\n",
    "        data_x.append(START_TOKEN + question + END_TOKEN)\n",
    "        data_y.append(START_TOKEN + answer + END_TOKEN)\n",
    "\n",
    "data_x = pad_sequences(data_x, padding='post', maxlen=MAX_LENGTH)\n",
    "data_y = pad_sequences(data_y, padding='post', maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(983, 50) (983, 50)\n"
     ]
    }
   ],
   "source": [
    "print(data_x.shape, data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tensorflow dataset for training purposes\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': data_x,\n",
    "        'dec_inputs': data_y[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': data_y[:, 1:]\n",
    "    }\n",
    "))\n",
    "\n",
    "dataset = dataset.cache() #creates a dataset in memory\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following layers and functions pulled from https://colab.research.google.com/github/tensorflow/examples/blob/master/community/en/transformer_chatbot.ipynb#scrollTo=L9eYssGIAG4h\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights. \"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # split heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # concatenation of heads\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                    (batch_size, -1, self.d_model))\n",
    "\n",
    "        # final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets rid of 0 tokens to not affect calculations\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a mask for predicting future words\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives words meaning based on position\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "        # apply sin to even index in the array\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # apply cos to odd index in the array\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives the input meaning to be used in the decoder\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combines previous functions to give words meaning\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoding layer uses encoded inputs to create new words\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': look_ahead_mask\n",
    "        })\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1,\n",
    "            'key': enc_outputs,\n",
    "            'value': enc_outputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "    # mask the future tokens for decoder inputs at the 1st attention block\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "    # mask the encoder outputs for the 2nd attention block\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a transformer model (change the values as needed)\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determines innaccuracy\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable learning rate to optimize model\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"transformer\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninputs (InputLayer)             [(None, None)]       0                                            \n__________________________________________________________________________________________________\ndec_inputs (InputLayer)         [(None, None)]       0                                            \n__________________________________________________________________________________________________\nenc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n__________________________________________________________________________________________________\nencoder (Functional)            (None, None, 256)    1645824     inputs[0][0]                     \n                                                                 enc_padding_mask[0][0]           \n__________________________________________________________________________________________________\nlook_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n__________________________________________________________________________________________________\ndec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n__________________________________________________________________________________________________\ndecoder (Functional)            (None, None, 256)    2173184     dec_inputs[0][0]                 \n                                                                 encoder[0][0]                    \n                                                                 look_ahead_mask[0][0]            \n                                                                 dec_padding_mask[0][0]           \n__________________________________________________________________________________________________\noutputs (Dense)                 (None, None, 2311)   593927      decoder[0][0]                    \n==================================================================================================\nTotal params: 4,412,935\nTrainable params: 4,412,935\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#put everything together\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 19s 865ms/step - loss: 1.8890 - accuracy: 7.0165e-05\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 14s 861ms/step - loss: 1.9075 - accuracy: 8.8311e-05\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 16s 1s/step - loss: 1.8052 - accuracy: 0.0023\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 17s 1s/step - loss: 1.7204 - accuracy: 0.0247\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 15s 958ms/step - loss: 1.6886 - accuracy: 0.0270\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 15s 971ms/step - loss: 1.6349 - accuracy: 0.0264\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 15s 969ms/step - loss: 1.5820 - accuracy: 0.0270\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 14s 872ms/step - loss: 1.5830 - accuracy: 0.0266\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 15s 940ms/step - loss: 1.5311 - accuracy: 0.0252\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 15s 912ms/step - loss: 1.5118 - accuracy: 0.0265\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 15s 951ms/step - loss: 1.4756 - accuracy: 0.0270\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 15s 902ms/step - loss: 1.4354 - accuracy: 0.0282\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 14s 878ms/step - loss: 1.4292 - accuracy: 0.0298\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 14s 882ms/step - loss: 1.4002 - accuracy: 0.0307\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 14s 895ms/step - loss: 1.3563 - accuracy: 0.0359\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 15s 916ms/step - loss: 1.3182 - accuracy: 0.0372\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 14s 870ms/step - loss: 1.3070 - accuracy: 0.0383\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 16s 1s/step - loss: 1.2888 - accuracy: 0.0401\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 18s 1s/step - loss: 1.2358 - accuracy: 0.0401\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 18s 1s/step - loss: 1.2527 - accuracy: 0.0427\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2488caab948>"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "#train the model\n",
    "EPOCHS = 20\n",
    "\n",
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a prediction\n",
    "def predict(sentence):\n",
    "    sentence = preprocess(sentence)\n",
    "\n",
    "    #tokenize the sentence\n",
    "    sentence = START_TOKEN + c_tokenizer.texts_to_sequences([sentence])[0] + END_TOKEN\n",
    "    #expand dimension to comply with the model\n",
    "    sentence = tf.expand_dims(sentence, axis=0)\n",
    "    #create a placeholder output\n",
    "    output = tf.expand_dims(START_TOKEN, axis=0)\n",
    "\n",
    "    #predict word by word\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        #stop if the sentence has reached END_TOKEN\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    #flatten the output and remove the first number\n",
    "    output = [tf.squeeze(output, axis=0).numpy().tolist()]\n",
    "    \n",
    "    #convert it to a string\n",
    "    output = c_tokenizer.sequences_to_texts(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[2309, 6, 2]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['thrown i .']"
      ]
     },
     "metadata": {},
     "execution_count": 235
    }
   ],
   "source": [
    "predict(\"What's good ma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}